{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_boston(return_X_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benchmark score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.647207423956008"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(y, [y.mean()]*y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    model = models.Sequential()\n",
    "\n",
    "    model.add(layers.Dense(10, activation='relu', input_dim=13)) # First hidden layer with 5 neurons\n",
    "    model.add(layers.Dense(7, activation='relu')) # First hidden layer with 5 neurons\n",
    "    model.add(layers.Dense(1, activation='linear')) \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(model, optimizer_name, *args):\n",
    "\n",
    "    loss = 'mae'\n",
    "    if args:\n",
    "        loss = args[0]\n",
    "\n",
    "    model.compile(optimizer=optimizer_name,\n",
    "                  loss=loss,\n",
    "                  metrics=['mae'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "16/16 [==============================] - 2s 8ms/step - loss: 18.7714 - mae: 18.7714 - val_loss: 16.7100 - val_mae: 16.7100\n",
      "Epoch 2/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 13.4152 - mae: 13.4152 - val_loss: 11.7580 - val_mae: 11.7580\n",
      "Epoch 3/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 9.8909 - mae: 9.8909 - val_loss: 8.3195 - val_mae: 8.3195\n",
      "Epoch 4/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 7.3018 - mae: 7.3018 - val_loss: 5.4761 - val_mae: 5.4761\n",
      "Epoch 5/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 6.2390 - mae: 6.2390 - val_loss: 5.1720 - val_mae: 5.1720\n",
      "Epoch 6/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.9693 - mae: 5.9693 - val_loss: 4.9394 - val_mae: 4.9394\n",
      "Epoch 7/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.8188 - mae: 5.8188 - val_loss: 4.7832 - val_mae: 4.7832\n",
      "Epoch 8/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.7037 - mae: 5.7037 - val_loss: 4.7803 - val_mae: 4.7803\n",
      "Epoch 9/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.6913 - mae: 5.6913 - val_loss: 4.6406 - val_mae: 4.6406\n",
      "Epoch 10/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.5239 - mae: 5.5239 - val_loss: 4.5661 - val_mae: 4.5661\n",
      "Epoch 11/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.4761 - mae: 5.4761 - val_loss: 4.7128 - val_mae: 4.7128\n",
      "Epoch 12/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.4789 - mae: 5.4789 - val_loss: 4.5067 - val_mae: 4.5067\n",
      "Epoch 13/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.2998 - mae: 5.2998 - val_loss: 4.5266 - val_mae: 4.5266\n",
      "Epoch 14/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.2716 - mae: 5.2716 - val_loss: 4.5399 - val_mae: 4.5399\n",
      "Epoch 15/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.2452 - mae: 5.2452 - val_loss: 4.6314 - val_mae: 4.6314\n",
      "Epoch 16/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.1774 - mae: 5.1774 - val_loss: 4.4679 - val_mae: 4.4679\n",
      "Epoch 17/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.1713 - mae: 5.1713 - val_loss: 4.5571 - val_mae: 4.5571\n",
      "Epoch 18/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.1977 - mae: 5.1977 - val_loss: 4.4893 - val_mae: 4.4893\n",
      "Epoch 19/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.1751 - mae: 5.1751 - val_loss: 4.4294 - val_mae: 4.4294\n",
      "Epoch 20/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.1487 - mae: 5.1487 - val_loss: 4.5608 - val_mae: 4.5608\n",
      "Epoch 21/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.2164 - mae: 5.2164 - val_loss: 4.4871 - val_mae: 4.4871\n",
      "Epoch 22/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.0657 - mae: 5.0657 - val_loss: 4.4120 - val_mae: 4.4120\n",
      "Epoch 23/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.0929 - mae: 5.0929 - val_loss: 4.4387 - val_mae: 4.4387\n",
      "Epoch 24/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.0402 - mae: 5.0402 - val_loss: 4.4194 - val_mae: 4.4194\n",
      "Epoch 25/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.0663 - mae: 5.0663 - val_loss: 4.4083 - val_mae: 4.4083\n",
      "Epoch 26/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.0555 - mae: 5.0555 - val_loss: 4.4932 - val_mae: 4.4932\n",
      "Epoch 27/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.1408 - mae: 5.1408 - val_loss: 4.3608 - val_mae: 4.3608\n",
      "Epoch 28/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.0866 - mae: 5.0866 - val_loss: 4.4455 - val_mae: 4.4455\n",
      "Epoch 29/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.2328 - mae: 5.2328 - val_loss: 4.6000 - val_mae: 4.6000\n",
      "Epoch 30/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.0428 - mae: 5.0428 - val_loss: 4.5178 - val_mae: 4.5178\n",
      "Epoch 31/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 5.0694 - mae: 5.0694 - val_loss: 4.4271 - val_mae: 4.4271\n",
      "Epoch 32/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.9617 - mae: 4.9617 - val_loss: 4.3723 - val_mae: 4.3723\n",
      "Epoch 33/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.9718 - mae: 4.9718 - val_loss: 4.3228 - val_mae: 4.3228\n",
      "Epoch 34/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.9175 - mae: 4.9175 - val_loss: 4.3094 - val_mae: 4.3094\n",
      "Epoch 35/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.9160 - mae: 4.9160 - val_loss: 4.3235 - val_mae: 4.3235\n",
      "Epoch 36/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.9080 - mae: 4.9080 - val_loss: 4.2808 - val_mae: 4.2808\n",
      "Epoch 37/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.8852 - mae: 4.8852 - val_loss: 4.3260 - val_mae: 4.3260\n",
      "Epoch 38/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.8709 - mae: 4.8709 - val_loss: 4.3020 - val_mae: 4.3020\n",
      "Epoch 39/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.8972 - mae: 4.8972 - val_loss: 4.3342 - val_mae: 4.3342\n",
      "Epoch 40/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.8848 - mae: 4.8848 - val_loss: 4.2991 - val_mae: 4.2991\n",
      "Epoch 41/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.8758 - mae: 4.8758 - val_loss: 4.3153 - val_mae: 4.3153\n",
      "Epoch 42/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.8912 - mae: 4.8912 - val_loss: 4.4353 - val_mae: 4.4353\n",
      "Epoch 43/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.8809 - mae: 4.8809 - val_loss: 4.2688 - val_mae: 4.2688\n",
      "Epoch 44/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.8457 - mae: 4.8457 - val_loss: 4.3230 - val_mae: 4.3230\n",
      "Epoch 45/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.8337 - mae: 4.8337 - val_loss: 4.3004 - val_mae: 4.3004\n",
      "Epoch 46/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.8423 - mae: 4.8423 - val_loss: 4.2975 - val_mae: 4.2975\n",
      "Epoch 47/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.7821 - mae: 4.7821 - val_loss: 4.3183 - val_mae: 4.3183\n",
      "Epoch 48/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.7951 - mae: 4.7951 - val_loss: 4.2909 - val_mae: 4.2909\n",
      "Epoch 49/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.7866 - mae: 4.7866 - val_loss: 4.2429 - val_mae: 4.2429\n",
      "Epoch 50/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.7521 - mae: 4.7521 - val_loss: 4.3017 - val_mae: 4.3017\n",
      "Epoch 51/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.7075 - mae: 4.7075 - val_loss: 4.3344 - val_mae: 4.3344\n",
      "Epoch 52/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.7391 - mae: 4.7391 - val_loss: 4.4395 - val_mae: 4.4395\n",
      "Epoch 53/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.9212 - mae: 4.9212 - val_loss: 4.2803 - val_mae: 4.2803\n",
      "Epoch 54/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.7306 - mae: 4.7306 - val_loss: 4.2289 - val_mae: 4.2289\n",
      "Epoch 55/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.7091 - mae: 4.7091 - val_loss: 4.1994 - val_mae: 4.1994\n",
      "Epoch 56/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.6682 - mae: 4.6682 - val_loss: 4.2202 - val_mae: 4.2202\n",
      "Epoch 57/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.6695 - mae: 4.6695 - val_loss: 4.3087 - val_mae: 4.3087\n",
      "Epoch 58/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.6898 - mae: 4.6898 - val_loss: 4.2371 - val_mae: 4.2371\n",
      "Epoch 59/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.7327 - mae: 4.7327 - val_loss: 4.1248 - val_mae: 4.1248\n",
      "Epoch 60/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.6863 - mae: 4.6863 - val_loss: 4.1998 - val_mae: 4.1998\n",
      "Epoch 61/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.6645 - mae: 4.6645 - val_loss: 4.1892 - val_mae: 4.1892\n",
      "Epoch 62/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.6458 - mae: 4.6458 - val_loss: 4.2693 - val_mae: 4.2693\n",
      "Epoch 63/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 4.5905 - mae: 4.5905 - val_loss: 4.1288 - val_mae: 4.1288\n",
      "Epoch 64/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.5882 - mae: 4.5882 - val_loss: 4.2400 - val_mae: 4.2400\n",
      "Epoch 65/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.6228 - mae: 4.6228 - val_loss: 4.1813 - val_mae: 4.1813\n",
      "Epoch 66/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.5630 - mae: 4.5630 - val_loss: 4.1272 - val_mae: 4.1272\n",
      "Epoch 67/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.6711 - mae: 4.6711 - val_loss: 4.2801 - val_mae: 4.2801\n",
      "Epoch 68/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.5216 - mae: 4.5216 - val_loss: 4.1732 - val_mae: 4.1732\n",
      "Epoch 69/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.5826 - mae: 4.5826 - val_loss: 4.1000 - val_mae: 4.1000\n",
      "Epoch 70/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.5731 - mae: 4.5731 - val_loss: 4.1233 - val_mae: 4.1233\n",
      "Epoch 71/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 4.5164 - mae: 4.5164 - val_loss: 4.1609 - val_mae: 4.1609\n",
      "Epoch 72/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.6900 - mae: 4.6900 - val_loss: 4.1711 - val_mae: 4.1711\n",
      "Epoch 73/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.4507 - mae: 4.4507 - val_loss: 4.1068 - val_mae: 4.1068\n",
      "Epoch 74/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.5291 - mae: 4.5291 - val_loss: 4.0240 - val_mae: 4.0240\n",
      "Epoch 75/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.4792 - mae: 4.4792 - val_loss: 4.1596 - val_mae: 4.1596\n",
      "Epoch 76/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.4636 - mae: 4.4636 - val_loss: 4.0862 - val_mae: 4.0862\n",
      "Epoch 77/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.4981 - mae: 4.4981 - val_loss: 4.1142 - val_mae: 4.1142\n",
      "Epoch 78/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.5091 - mae: 4.5091 - val_loss: 4.1529 - val_mae: 4.1529\n",
      "Epoch 79/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.5148 - mae: 4.5148 - val_loss: 4.0305 - val_mae: 4.0305\n",
      "Epoch 80/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3864 - mae: 4.3864 - val_loss: 4.0343 - val_mae: 4.0343\n",
      "Epoch 81/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.4274 - mae: 4.4274 - val_loss: 4.0673 - val_mae: 4.0673\n",
      "Epoch 82/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.4276 - mae: 4.4276 - val_loss: 4.0236 - val_mae: 4.0236\n",
      "Epoch 83/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3713 - mae: 4.3713 - val_loss: 4.0469 - val_mae: 4.0469\n",
      "Epoch 84/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.4105 - mae: 4.4105 - val_loss: 4.0330 - val_mae: 4.0330\n",
      "Epoch 85/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3827 - mae: 4.3827 - val_loss: 4.0863 - val_mae: 4.0863\n",
      "Epoch 86/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3813 - mae: 4.3813 - val_loss: 4.0290 - val_mae: 4.0290\n",
      "Epoch 87/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3854 - mae: 4.3854 - val_loss: 4.0554 - val_mae: 4.0554\n",
      "Epoch 88/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.4209 - mae: 4.4209 - val_loss: 4.0999 - val_mae: 4.0999\n",
      "Epoch 89/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.4536 - mae: 4.4536 - val_loss: 4.0354 - val_mae: 4.0354\n",
      "Epoch 90/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3114 - mae: 4.3114 - val_loss: 3.9742 - val_mae: 3.9742\n",
      "Epoch 91/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3330 - mae: 4.3330 - val_loss: 3.9768 - val_mae: 3.9768\n",
      "Epoch 92/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.2724 - mae: 4.2724 - val_loss: 4.0040 - val_mae: 4.0040\n",
      "Epoch 93/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3243 - mae: 4.3243 - val_loss: 4.0163 - val_mae: 4.0163\n",
      "Epoch 94/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.2754 - mae: 4.2754 - val_loss: 3.9969 - val_mae: 3.9969\n",
      "Epoch 95/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3025 - mae: 4.3025 - val_loss: 3.9919 - val_mae: 3.9919\n",
      "Epoch 96/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.2711 - mae: 4.2711 - val_loss: 4.0228 - val_mae: 4.0228\n",
      "Epoch 97/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.2517 - mae: 4.2517 - val_loss: 3.9566 - val_mae: 3.9566\n",
      "Epoch 98/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.2000 - mae: 4.2000 - val_loss: 3.9696 - val_mae: 3.9696\n",
      "Epoch 99/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3571 - mae: 4.3571 - val_loss: 4.1791 - val_mae: 4.1791\n",
      "Epoch 100/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.2169 - mae: 4.2169 - val_loss: 3.9083 - val_mae: 3.9083\n",
      "Epoch 101/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.2225 - mae: 4.2225 - val_loss: 3.9672 - val_mae: 3.9672\n",
      "Epoch 102/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.2860 - mae: 4.2860 - val_loss: 3.9725 - val_mae: 3.9725\n",
      "Epoch 103/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.2423 - mae: 4.2423 - val_loss: 3.9066 - val_mae: 3.9066\n",
      "Epoch 104/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.2200 - mae: 4.2200 - val_loss: 3.9309 - val_mae: 3.9309\n",
      "Epoch 105/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3116 - mae: 4.3116 - val_loss: 4.2083 - val_mae: 4.2083\n",
      "Epoch 106/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.2723 - mae: 4.2723 - val_loss: 4.0785 - val_mae: 4.0785\n",
      "Epoch 107/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.2252 - mae: 4.2252 - val_loss: 4.0515 - val_mae: 4.0515\n",
      "Epoch 108/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.3726 - mae: 4.3726 - val_loss: 3.9250 - val_mae: 3.9250\n",
      "Epoch 109/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.1621 - mae: 4.1621 - val_loss: 3.9103 - val_mae: 3.9103\n",
      "Epoch 110/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.1392 - mae: 4.1392 - val_loss: 3.8751 - val_mae: 3.8751\n",
      "Epoch 111/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.1525 - mae: 4.1525 - val_loss: 3.9289 - val_mae: 3.9289\n",
      "Epoch 112/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.1724 - mae: 4.1724 - val_loss: 3.8113 - val_mae: 3.8113\n",
      "Epoch 113/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.1275 - mae: 4.1275 - val_loss: 3.9245 - val_mae: 3.9245\n",
      "Epoch 114/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.1675 - mae: 4.1675 - val_loss: 3.9203 - val_mae: 3.9203\n",
      "Epoch 115/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.1120 - mae: 4.1120 - val_loss: 3.7942 - val_mae: 3.7942\n",
      "Epoch 116/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.0971 - mae: 4.0971 - val_loss: 3.9346 - val_mae: 3.9346\n",
      "Epoch 117/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.1939 - mae: 4.1939 - val_loss: 3.7869 - val_mae: 3.7869\n",
      "Epoch 118/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.0882 - mae: 4.0882 - val_loss: 3.8660 - val_mae: 3.8660\n",
      "Epoch 119/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.0946 - mae: 4.0946 - val_loss: 3.7994 - val_mae: 3.7994\n",
      "Epoch 120/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.0590 - mae: 4.0590 - val_loss: 3.8375 - val_mae: 3.8375\n",
      "Epoch 121/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.0362 - mae: 4.0362 - val_loss: 3.7619 - val_mae: 3.7619\n",
      "Epoch 122/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.1103 - mae: 4.1103 - val_loss: 3.8203 - val_mae: 3.8203\n",
      "Epoch 123/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.0454 - mae: 4.0454 - val_loss: 3.8679 - val_mae: 3.8679\n",
      "Epoch 124/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.0493 - mae: 4.0493 - val_loss: 3.9580 - val_mae: 3.9580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 125/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.0488 - mae: 4.0488 - val_loss: 3.8902 - val_mae: 3.8902\n",
      "Epoch 126/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.9855 - mae: 3.9855 - val_loss: 3.8164 - val_mae: 3.8164\n",
      "Epoch 127/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.0249 - mae: 4.0249 - val_loss: 3.8104 - val_mae: 3.8104\n",
      "Epoch 128/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.0122 - mae: 4.0122 - val_loss: 3.8712 - val_mae: 3.8712\n",
      "Epoch 129/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.0414 - mae: 4.0414 - val_loss: 3.7994 - val_mae: 3.7994\n",
      "Epoch 130/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.9905 - mae: 3.9905 - val_loss: 3.8624 - val_mae: 3.8624\n",
      "Epoch 131/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.0890 - mae: 4.0890 - val_loss: 3.9112 - val_mae: 3.9112\n",
      "Epoch 132/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.9553 - mae: 3.9553 - val_loss: 3.7877 - val_mae: 3.7877\n",
      "Epoch 133/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.9821 - mae: 3.9821 - val_loss: 3.7674 - val_mae: 3.7674\n",
      "Epoch 134/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.9435 - mae: 3.9435 - val_loss: 3.7540 - val_mae: 3.7540\n",
      "Epoch 135/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.0098 - mae: 4.0098 - val_loss: 3.7518 - val_mae: 3.7518\n",
      "Epoch 136/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.9363 - mae: 3.9363 - val_loss: 3.7468 - val_mae: 3.7468\n",
      "Epoch 137/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.9936 - mae: 3.9936 - val_loss: 3.8990 - val_mae: 3.8990\n",
      "Epoch 138/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.9717 - mae: 3.9717 - val_loss: 3.7097 - val_mae: 3.7097\n",
      "Epoch 139/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.9856 - mae: 3.9856 - val_loss: 3.8732 - val_mae: 3.8732\n",
      "Epoch 140/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.9333 - mae: 3.9333 - val_loss: 3.7894 - val_mae: 3.7894\n",
      "Epoch 141/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.9743 - mae: 3.9743 - val_loss: 3.8069 - val_mae: 3.8069\n",
      "Epoch 142/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.8906 - mae: 3.8906 - val_loss: 3.6676 - val_mae: 3.6676\n",
      "Epoch 143/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.8844 - mae: 3.8844 - val_loss: 3.6747 - val_mae: 3.6747\n",
      "Epoch 144/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 4.0309 - mae: 4.0309 - val_loss: 3.9975 - val_mae: 3.9975\n",
      "Epoch 145/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.9207 - mae: 3.9207 - val_loss: 3.6719 - val_mae: 3.6719\n",
      "Epoch 146/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.8401 - mae: 3.8401 - val_loss: 3.6605 - val_mae: 3.6605\n",
      "Epoch 147/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.9274 - mae: 3.9274 - val_loss: 3.6661 - val_mae: 3.6661\n",
      "Epoch 148/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.8174 - mae: 3.8174 - val_loss: 3.7062 - val_mae: 3.7062\n",
      "Epoch 149/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.9142 - mae: 3.9142 - val_loss: 3.7191 - val_mae: 3.7191\n",
      "Epoch 150/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.8359 - mae: 3.8359 - val_loss: 3.6442 - val_mae: 3.6442\n",
      "Epoch 151/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.8550 - mae: 3.8550 - val_loss: 3.7333 - val_mae: 3.7333\n",
      "Epoch 152/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.8165 - mae: 3.8165 - val_loss: 3.6931 - val_mae: 3.6931\n",
      "Epoch 153/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.8086 - mae: 3.8086 - val_loss: 3.6655 - val_mae: 3.6655\n",
      "Epoch 154/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7743 - mae: 3.7743 - val_loss: 3.6595 - val_mae: 3.6595\n",
      "Epoch 155/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.8080 - mae: 3.8080 - val_loss: 3.7141 - val_mae: 3.7141\n",
      "Epoch 156/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.8938 - mae: 3.8938 - val_loss: 3.6830 - val_mae: 3.6830\n",
      "Epoch 157/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7710 - mae: 3.7710 - val_loss: 3.6422 - val_mae: 3.6422\n",
      "Epoch 158/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7705 - mae: 3.7705 - val_loss: 3.6850 - val_mae: 3.6850\n",
      "Epoch 159/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7968 - mae: 3.7968 - val_loss: 3.6667 - val_mae: 3.6667\n",
      "Epoch 160/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.8018 - mae: 3.8018 - val_loss: 3.6791 - val_mae: 3.6791\n",
      "Epoch 161/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7455 - mae: 3.7455 - val_loss: 3.6870 - val_mae: 3.6870\n",
      "Epoch 162/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7533 - mae: 3.7533 - val_loss: 3.6941 - val_mae: 3.6941\n",
      "Epoch 163/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7637 - mae: 3.7637 - val_loss: 3.7925 - val_mae: 3.7925\n",
      "Epoch 164/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7376 - mae: 3.7376 - val_loss: 3.7417 - val_mae: 3.7417\n",
      "Epoch 165/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7409 - mae: 3.7409 - val_loss: 3.6640 - val_mae: 3.6640\n",
      "Epoch 166/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7232 - mae: 3.7232 - val_loss: 3.6753 - val_mae: 3.6753\n",
      "Epoch 167/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.8177 - mae: 3.8177 - val_loss: 3.7141 - val_mae: 3.7141\n",
      "Epoch 168/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7609 - mae: 3.7609 - val_loss: 3.6669 - val_mae: 3.6669\n",
      "Epoch 169/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7024 - mae: 3.7024 - val_loss: 3.9193 - val_mae: 3.9193\n",
      "Epoch 170/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7878 - mae: 3.7878 - val_loss: 3.7103 - val_mae: 3.7103\n",
      "Epoch 171/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7827 - mae: 3.7827 - val_loss: 3.7755 - val_mae: 3.7755\n",
      "Epoch 172/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.8231 - mae: 3.8231 - val_loss: 3.7143 - val_mae: 3.7143\n",
      "Epoch 173/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7622 - mae: 3.7622 - val_loss: 3.6716 - val_mae: 3.6716\n",
      "Epoch 174/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7174 - mae: 3.7174 - val_loss: 3.7492 - val_mae: 3.7492\n",
      "Epoch 175/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6860 - mae: 3.6860 - val_loss: 3.7307 - val_mae: 3.7307\n",
      "Epoch 176/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7156 - mae: 3.7156 - val_loss: 3.7417 - val_mae: 3.7417\n",
      "Epoch 177/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6748 - mae: 3.6748 - val_loss: 3.6546 - val_mae: 3.6546\n",
      "Epoch 178/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.8123 - mae: 3.8123 - val_loss: 3.6927 - val_mae: 3.6927\n",
      "Epoch 179/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6619 - mae: 3.6619 - val_loss: 3.7763 - val_mae: 3.7763\n",
      "Epoch 180/500\n",
      "16/16 [==============================] - 0s 4ms/step - loss: 3.6666 - mae: 3.6666 - val_loss: 3.7061 - val_mae: 3.7061\n",
      "Epoch 181/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6415 - mae: 3.6415 - val_loss: 3.7798 - val_mae: 3.7798\n",
      "Epoch 182/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6653 - mae: 3.6653 - val_loss: 3.7414 - val_mae: 3.7414\n",
      "Epoch 183/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6755 - mae: 3.6755 - val_loss: 3.7029 - val_mae: 3.7029\n",
      "Epoch 184/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6475 - mae: 3.6475 - val_loss: 3.7823 - val_mae: 3.7823\n",
      "Epoch 185/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6954 - mae: 3.6954 - val_loss: 3.6312 - val_mae: 3.6312\n",
      "Epoch 186/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6081 - mae: 3.6081 - val_loss: 3.7721 - val_mae: 3.7721\n",
      "Epoch 187/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6515 - mae: 3.6515 - val_loss: 3.7052 - val_mae: 3.7052\n",
      "Epoch 188/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.5984 - mae: 3.5984 - val_loss: 3.6552 - val_mae: 3.6552\n",
      "Epoch 189/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6075 - mae: 3.6075 - val_loss: 3.6902 - val_mae: 3.6902\n",
      "Epoch 190/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7153 - mae: 3.7153 - val_loss: 3.7225 - val_mae: 3.7225\n",
      "Epoch 191/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6924 - mae: 3.6924 - val_loss: 3.7432 - val_mae: 3.7432\n",
      "Epoch 192/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6195 - mae: 3.6195 - val_loss: 3.7771 - val_mae: 3.7771\n",
      "Epoch 193/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.8125 - mae: 3.8125 - val_loss: 3.6909 - val_mae: 3.6909\n",
      "Epoch 194/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6143 - mae: 3.6143 - val_loss: 3.7226 - val_mae: 3.7226\n",
      "Epoch 195/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.5919 - mae: 3.5919 - val_loss: 3.7155 - val_mae: 3.7155\n",
      "Epoch 196/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7274 - mae: 3.7274 - val_loss: 3.7423 - val_mae: 3.7423\n",
      "Epoch 197/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6550 - mae: 3.6550 - val_loss: 3.7666 - val_mae: 3.7666\n",
      "Epoch 198/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6147 - mae: 3.6147 - val_loss: 3.7376 - val_mae: 3.7376\n",
      "Epoch 199/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6276 - mae: 3.6276 - val_loss: 3.6449 - val_mae: 3.6449\n",
      "Epoch 200/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6229 - mae: 3.6229 - val_loss: 3.6700 - val_mae: 3.6700\n",
      "Epoch 201/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.6090 - mae: 3.6090 - val_loss: 3.6879 - val_mae: 3.6879\n",
      "Epoch 202/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.5891 - mae: 3.5891 - val_loss: 3.6770 - val_mae: 3.6770\n",
      "Epoch 203/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.5990 - mae: 3.5990 - val_loss: 3.7705 - val_mae: 3.7705\n",
      "Epoch 204/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.5435 - mae: 3.5435 - val_loss: 4.0965 - val_mae: 4.0965\n",
      "Epoch 205/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7870 - mae: 3.7870 - val_loss: 3.6179 - val_mae: 3.6179\n",
      "Epoch 206/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.5618 - mae: 3.5618 - val_loss: 3.7802 - val_mae: 3.7802\n",
      "Epoch 207/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.5855 - mae: 3.5855 - val_loss: 3.7106 - val_mae: 3.7106\n",
      "Epoch 208/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.5689 - mae: 3.5689 - val_loss: 3.7266 - val_mae: 3.7266\n",
      "Epoch 209/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.5694 - mae: 3.5694 - val_loss: 3.6770 - val_mae: 3.6770\n",
      "Epoch 210/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.5104 - mae: 3.5104 - val_loss: 3.7952 - val_mae: 3.7952\n",
      "Epoch 211/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.5798 - mae: 3.5798 - val_loss: 3.9327 - val_mae: 3.9327\n",
      "Epoch 212/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.7000 - mae: 3.7000 - val_loss: 3.6629 - val_mae: 3.6629\n",
      "Epoch 213/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4849 - mae: 3.4849 - val_loss: 3.6832 - val_mae: 3.6832\n",
      "Epoch 214/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.5138 - mae: 3.5138 - val_loss: 3.7117 - val_mae: 3.7117\n",
      "Epoch 215/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.5789 - mae: 3.5789 - val_loss: 3.6462 - val_mae: 3.6462\n",
      "Epoch 216/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4841 - mae: 3.4841 - val_loss: 3.7794 - val_mae: 3.7794\n",
      "Epoch 217/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.5329 - mae: 3.5329 - val_loss: 3.6990 - val_mae: 3.6990\n",
      "Epoch 218/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4890 - mae: 3.4890 - val_loss: 3.6769 - val_mae: 3.6769\n",
      "Epoch 219/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.5140 - mae: 3.5140 - val_loss: 3.6915 - val_mae: 3.6915\n",
      "Epoch 220/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4825 - mae: 3.4825 - val_loss: 3.6946 - val_mae: 3.6946\n",
      "Epoch 221/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4435 - mae: 3.4435 - val_loss: 3.6632 - val_mae: 3.6632\n",
      "Epoch 222/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4800 - mae: 3.4800 - val_loss: 3.6325 - val_mae: 3.6325\n",
      "Epoch 223/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4405 - mae: 3.4405 - val_loss: 3.7727 - val_mae: 3.7727\n",
      "Epoch 224/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4534 - mae: 3.4534 - val_loss: 3.7091 - val_mae: 3.7091\n",
      "Epoch 225/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.5113 - mae: 3.5113 - val_loss: 3.7184 - val_mae: 3.7184\n",
      "Epoch 226/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4374 - mae: 3.4374 - val_loss: 3.6662 - val_mae: 3.6662\n",
      "Epoch 227/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.5068 - mae: 3.5068 - val_loss: 3.6538 - val_mae: 3.6538\n",
      "Epoch 228/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4430 - mae: 3.4430 - val_loss: 3.6418 - val_mae: 3.6418\n",
      "Epoch 229/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4825 - mae: 3.4825 - val_loss: 3.6731 - val_mae: 3.6731\n",
      "Epoch 230/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4532 - mae: 3.4532 - val_loss: 3.6505 - val_mae: 3.6505\n",
      "Epoch 231/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4662 - mae: 3.4662 - val_loss: 3.7251 - val_mae: 3.7251\n",
      "Epoch 232/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4734 - mae: 3.4734 - val_loss: 3.7990 - val_mae: 3.7990\n",
      "Epoch 233/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.5785 - mae: 3.5785 - val_loss: 3.7493 - val_mae: 3.7493\n",
      "Epoch 234/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4776 - mae: 3.4776 - val_loss: 3.6373 - val_mae: 3.6373\n",
      "Epoch 235/500\n",
      "16/16 [==============================] - 0s 2ms/step - loss: 3.4163 - mae: 3.4163 - val_loss: 3.6395 - val_mae: 3.6395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4.311474800109863, 4.311474800109863]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es = EarlyStopping(patience=30, restore_best_weights=True)\n",
    "\n",
    "model = initialize_model()\n",
    "\n",
    "model = compile_model(model, 'adam')\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.3,\n",
    "                    epochs=500,\n",
    "                    batch_size=16, \n",
    "                    verbose=1, \n",
    "                    callbacks=[es])\n",
    "\n",
    "model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()\n",
    "\n",
    "model = compile_model(model, opt)\n",
    "history = model.fit(X_train, y_train,\n",
    "                    validation_split=0.3,\n",
    "                    epochs=500,\n",
    "                    batch_size=16, \n",
    "                    verbose=0, \n",
    "                    callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exponential decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.001\n",
    "\n",
    "lr_schedule = ExponentialDecay(\n",
    "    initial_learning_rate, decay_steps=5000, decay_rate=0.7,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()\n",
    "\n",
    "opt = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model = compile_model(model, opt)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                validation_split=0.3,\n",
    "                epochs=500,\n",
    "                batch_size=16, \n",
    "                verbose=0, \n",
    "                callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()\n",
    "\n",
    "opt = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model_mse = compile_model(model, opt, 'mse')\n",
    "\n",
    "history_mse = model_mse.fit(X_train, y_train,\n",
    "                validation_split=0.3,\n",
    "                epochs=500,\n",
    "                batch_size=16, \n",
    "                verbose=0, \n",
    "                callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = initialize_model()\n",
    "\n",
    "opt = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model = compile_model(model, opt)\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                validation_split=0.3,\n",
    "                epochs=500,\n",
    "                batch_size=16, \n",
    "                verbose=0, \n",
    "                callbacks=[es])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.322746583769994\n",
      "3.8508134658824016\n",
      "51.49620100855827\n",
      "30.733288884162903\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(history.history['loss']))\n",
    "print(np.mean(history.history['val_loss']))\n",
    "print(np.mean(history_mse.history['loss']))\n",
    "print(np.mean(history_mse.history['val_loss']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25.20331382751465, 3.811218738555908]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### YOUR CODE HERE\n",
    "model = initialize_model()\n",
    "\n",
    "opt = Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model = compile_model(model, opt, 'mse')\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                validation_split=0.3,\n",
    "                epochs=500,\n",
    "                batch_size=16, \n",
    "                verbose=0, \n",
    "                callbacks=[es])\n",
    "\n",
    "model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model/assets\n"
     ]
    }
   ],
   "source": [
    "models.save_model(model, 'my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = models.load_model('my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pm455OX6ksyl"
   },
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model_2():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(1000, activation='relu', input_dim=13))\n",
    "    model.add(layers.Dense(750, activation='relu'))\n",
    "    model.add(layers.Dense(500, activation='relu'))\n",
    "    model.add(layers.Dense(500, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = initialize_model_2()\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data = (X_test, y_test),\n",
    "                  batch_size=16, \n",
    "                  epochs=1000, \n",
    "                  verbose=0)\n",
    "\n",
    "res = model.evaluate(X_test, y_test, verbose=0)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gRMeffBOksys"
   },
   "outputs": [],
   "source": [
    "def initialize_model_2():\n",
    "    reg_l1 = regularizers.L1(0.01)\n",
    "    reg_l2 = regularizers.L2(0.01)\n",
    "    reg_l1_l2 = regularizers.l1_l2(l1=0.005, l2=0.0005)\n",
    "\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    model.add(layers.Dense(1000, activation='relu', input_dim=13))\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "    model.add(layers.Dense(750, activation='relu', kernel_regularizer=reg_l1))\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "    model.add(layers.Dense(500, activation='relu', bias_regularizer=reg_l2))\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "    model.add(layers.Dense(500, activation='relu', activity_regularizer=reg_l1_l2))\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_1 = initialize_model_2()\n",
    "\n",
    "history_1 = model_1.fit(X_train, y_train, \n",
    "                    validation_data = (X_test, y_test),\n",
    "                  batch_size=16, \n",
    "                  epochs=1000, \n",
    "                  verbose=0)\n",
    "\n",
    "res_1 = model_1.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkKk_3Keksyw"
   },
   "source": [
    "## Best MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorflow.python.keras.regularizers.L1 object at 0x7f8bd3a7ceb0>\n"
     ]
    }
   ],
   "source": [
    "def initialize_model_3():\n",
    "    \n",
    "    reg_l1 = regularizers.L1(0.01)\n",
    "    reg_l2 = regularizers.L2(0.01)\n",
    "    reg_l1_l2 = regularizers.l1_l2(l1=0.005, l2=0.0005)\n",
    "\n",
    "    normalizer = Normalization() # Instantiate a \"normalizer\" layer\n",
    "    normalizer.adapt(X_train) # \"Fit\" it on the train set\n",
    "    \n",
    "    model = models.Sequential(normalizer)\n",
    "    model.add(layers.Dense(400, activation='relu', input_dim=13))\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "    model.add(layers.Dense(100, activation='relu', kernel_regularizer=reg_l1))\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "    model.add(layers.Dense(100, activation='relu', bias_regularizer=reg_l2))\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "\n",
    "    model.add(layers.Dense(100, activation='relu', activity_regularizer=reg_l1_l2))\n",
    "    model.add(layers.Dropout(rate=0.2))\n",
    "    \n",
    "    model.add(layers.Dense(1, activation='linear'))\n",
    "    \n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = initialize_model_3()\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    validation_data = (X_test, y_test),\n",
    "                  batch_size=16, \n",
    "                  epochs=1000, \n",
    "                  verbose=0)\n",
    "\n",
    "res_2 = model.evaluate(X_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14.4872407913208, 2.1154489517211914]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res_2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
